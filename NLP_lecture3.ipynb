{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-lecture3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN45vMozNrso6J2fgvRbTuG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saraelshawa/CSC324/blob/master/NLP_lecture3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzbdrQDVnFVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeelJiWFnJKH",
        "colab_type": "text"
      },
      "source": [
        "# Lecture 3 \n",
        "\n",
        "## conditioned language models\n",
        "- not just generate text, generate text according to some specification\n",
        "\n",
        "Input x --> Output Y (Text) ---> Task \n",
        "\n",
        "structured data -> nl description --> NL Generation (data-to-text generation) how you get weather reports, responses from google assistant\n",
        "\n",
        "english --> japanese --> Translation \n",
        "\n",
        "document --> short description --> Summarization\n",
        "\n",
        "utterance --> response  --> Response generation\n",
        "\n",
        "image --> text ---> image captioning\n",
        "\n",
        "speech --> transcript --> speech recognition \n",
        "\n",
        "they all follow the same format, so we can use very similar models\n",
        "\n",
        "## Calculating probability of a sentence\n",
        "- P(x) prob of next word, given the context \n",
        "## condition language models\n",
        "- P(y|x) we condition on input x and all the previous tokens until j-1 to predict j, x is added context\n",
        "\n",
        "## One Type ofLanguage Model Mikolov et al. 2011\n",
        "going through multiple LSTMs (sutskever et al 2014)\n",
        "for translation, we feed in whatever we want to translate and end of sentence symbol (**encoder**), then we predict all of the next words and generate outputs(**decoder**).\n",
        "\n",
        "ex. feed in japanese sentence, outputs english sentence\n",
        "\n",
        "# Methods of Generation\n",
        "## the generation model\n",
        "- we have a model of P(Y|X), how do we use it to generate a sentence \n",
        "- Two methods:\n",
        "    - **Sampling**: try to generate a random sentence accfording to prob distrub, when you neeed to generate a variety of different sentences, ex. chatbot and instead of when the user says hi, it would sometimes say hi, hello, greetings. Another reason is to see what the model thinks are reasonable sentence. if uyou only pick the best sentence you wont see what the model is thinking\n",
        "    - **Argmax**: try to generate the sentence with the highest probability. example in translation you will only show the user the best output and only one output\n",
        "  \n",
        "## Ancestral Sampling:\n",
        "- Randomly generate words one-by-one\n",
        "- while we havent reach end of sentence sybmol, calculate probability dist over the next word and randomly sample a word from there\n",
        "- an exact method for sampling p(y|x), no further work needed\n",
        "\n",
        "## Greedy Search (searching for the best output)\n",
        "- one by one, pich the single highest probability word\n",
        "- while we havent reach end of sentence symbol, pick highest scoring word given our context\n",
        "- not exact, real problems:\n",
        "    - will often generate the 'easy' words first\n",
        "    - will prefer multiple common words to one rare word (example: will choose\" \"new\" if you have New Jersey, New York, compared to Pittsburgh)\n",
        "    - you will end up getting suboptimal solutions doing this\n",
        "\n",
        "## Beam Search \n",
        "- instead of picking one high-probability word, maintain several paths (in the locations example, we would pick \"new\" and \"pittsburgh\", so not just \"new\" like in our greedy search \n",
        "- can find shorter sentences\n",
        "\n",
        "# How do we Evaluate? \n",
        "## human evaluation\n",
        "- as a human to do evaluation\n",
        "- is it adequate? (meaning correct) \n",
        "- is it fluent? (is the grammar correct)\n",
        "- betteR? rank-based evaluation. being adequate and not fluent is ok \"the bob visited the mary\", other way around no, \"mary visited bob\"\n",
        "- inal goal, but slow,expensice and sometimes inconsisten so we use automatic evaluation\n",
        "\n",
        "##basic evaluation paradigm automatic evaluiation\n",
        "- use parallel test set \n",
        "- use system to generate translations\n",
        "- compare target translations w/ reference\n",
        "\n",
        "## Bleu (first popular machine evalution)\n",
        "- works by comparing n-gram overlap w/reference\n",
        "-the more thing that are included in the system output that are also in the reference will increase the score\n",
        "- compare system output with reference and look for sufface-level match\n",
        "- pros: easy to use good for measuring system  improvement\n",
        "- cons: often doesn't match human eval, bad for comparing v different systems\n",
        "\n",
        "## METEOR\n",
        "- like bleu in overall principle, with many other tricks, like consider paraphrases, reordering, and function word/content word diff\n",
        "- pros: generally significantly better than bleu, esp for high-resource languages\n",
        "- cons: requires extra resources for new languages (although these can be made automatically), and more complicated \n",
        "\n",
        "## Embedding-based Metrics (BERTScore, BLEURT)\n",
        "- use either similarity between nerual net embeddings (bertscorer) or a trained nerual network model to predict human scores (BLEURT), or use neural net to predict what a human would say \n",
        "- pros: strong performance (at least on in-domain data) \n",
        "- cons: required trained embeddings, is not interpretable and may not generalize to new domains\n",
        "\n",
        "##perplexity (another method to test our models)\n",
        "- calculat ethe perplexity of the words in theheld-out set without doing generation\n",
        "- Pros: naturally solves multiple-reference problem!\n",
        "- Cons: doesn't consider decoding or actually generating output. so if you had a bug you cannot detect it with perplexity\n",
        "- maybe reasonable for problems with lots of ambiguity like dialogue\n",
        "\n",
        "# Debugging Generation\n",
        "## Debugging Search (1)\n",
        "- your decoding code (search code) should get the same score as loss calculation\n",
        "- test this:\n",
        "  -calculate loss of reference\n",
        "  - perform forced deocoding, where you decode, but tell your model the reference word at each tiem step\n",
        "  - the score of these two should be the same \n",
        "\n",
        "## (2)\n",
        "- as you make search better, the modeol score should get  better  (almost all time)\n",
        "- search with varying beam sizes and make sure you get a better overall model score with larger sizes \n",
        "\n",
        "## Look at your data!\n",
        "- decoding problems can often be detected by looking at outputs and realizing something is weong\n",
        "- ex. the first word of the sentence is droppped every time\n",
        "  - went to he store yesterday\n",
        "  - bought a dog\n",
        "\n",
        "##Quantitative analysis\n",
        "- measure gains quanititaitvely. what is the phenonmenon you chose to focus on? is that phenonmenon getting better? \n",
        "  - you focused on low-ffrequency words: is accuracy on low freqyency words increasing?\n",
        "  - you focused on syntax: is syntax or word ordering getting better, are you doing better on long-distance dependencies? \n",
        "  - you focused on search: are you reducing the number of search errors\n",
        "\n",
        "toolkit: neulab/compare-mt, automatically generates html report, outputs accuracies\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG7jfCZIvo00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}