{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-lecture3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOApLD0fPEwud+/urjmL4B9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saraelshawa/CSC324/blob/master/NLP_lecture3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzbdrQDVnFVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeelJiWFnJKH",
        "colab_type": "text"
      },
      "source": [
        "# Lecture 3 \n",
        "\n",
        "## conditioned language models\n",
        "- not just generate text, generate text according to some specification\n",
        "\n",
        "Input x --> Output Y (Text) ---> Task \n",
        "\n",
        "structured data -> nl description --> NL Generation (data-to-text generation) how you get weather reports, responses from google assistant\n",
        "\n",
        "english --> japanese --> Translation \n",
        "\n",
        "document --> short description --> Summarization\n",
        "\n",
        "utterance --> response  --> Response generation\n",
        "\n",
        "image --> text ---> image captioning\n",
        "\n",
        "speech --> transcript --> speech recognition \n",
        "\n",
        "they all follow the same format, so we can use very similar models\n",
        "\n",
        "## Calculating probability of a sentence\n",
        "- P(x) prob of next word, given the context \n",
        "## condition language models\n",
        "- P(y|x) we condition on input x and all the previous tokens until j-1 to predict j, x is added context\n",
        "\n",
        "## One Type ofLanguage Model Mikolov et al. 2011\n",
        "going through multiple LSTMs (sutskever et al 2014)\n",
        "for translation, we feed in whatever we want to translate and end of sentence symbol (**encoder**), then we predict all of the next words and generate outputs(**decoder**).\n",
        "\n",
        "ex. feed in japanese sentence, outputs english sentence\n",
        "\n",
        "# Methods of Generation\n",
        "## the generation model\n",
        "- we have a model of P(Y|X), how do we use it to generate a sentence \n",
        "- Two methods:\n",
        "    - **Sampling**: try to generate a random sentence accfording to prob distrub, when you neeed to generate a variety of different sentences, ex. chatbot and instead of when the user says hi, it would sometimes say hi, hello, greetings. Another reason is to see what the model thinks are reasonable sentence. if uyou only pick the best sentence you wont see what the model is thinking\n",
        "    - **Argmax**: try to generate the sentence with the highest probability. example in translation you will only show the user the best output and only one output\n",
        "  \n",
        "## Ancestral Sampling:\n",
        "- Randomly generate words one-by-one\n",
        "- while we havent reach end of sentence sybmol, calculate probability dist over the next word and randomly sample a word from there\n",
        "- an exact method for sampling p(y|x), no further work needed\n",
        "\n",
        "## Greedy Search (searching for the best output)\n",
        "- one by one, pich the single highest probability word\n",
        "- while we havent reach end of sentence symbol, pick highest scoring word given our context\n",
        "- not exact, real problems:\n",
        "    - will often generate the 'easy' words first\n",
        "    - will prefer multiple common words to one rare word (example: will choose\" \"new\" if you have New Jersey, New York, compared to Pittsburgh)\n",
        "    - you will end up getting suboptimal solutions doing this\n",
        "\n",
        "## Beam Search \n",
        "- instead of picking one high-probability word, maintain several paths (in the locations example, we would pick \"new\" and \"pittsburgh\", so not just \"new\" like in our greedy search \n",
        "- can find shorter sentences\n",
        "\n",
        "# How do we Evaluate? \n",
        "## human evaluation\n",
        "- as a human to do evaluation\n",
        "- is it adequate? (meaning correct) \n",
        "- is it fluent? (is the grammar correct)\n",
        "- betteR? rank-based evaluation. being adequate and not fluent is ok \"the bob visited the mary\", other way around no, \"mary visited bob\"\n",
        "- inal goal, but slow,expensice and sometimes inconsisten so we use automatic evaluation\n",
        "\n",
        "##basic evaluation paradigm automatic evaluiation\n",
        "- use parallel test set \n",
        "- use system to generate translations\n",
        "- compare target translations w/ reference\n",
        "\n",
        "## Bleu (first popular machine evalution)\n",
        "- works by comparing n-gram overlap w/reference\n",
        "-the more thing that are included in the system output that are also in the reference will increase the score\n",
        "- compare system output with reference and look for sufface-level match\n",
        "- pros: easy to use good for measuring system  improvement\n",
        "- cons: often doesn't match human eval, bad for comparing v different systems\n",
        "\n",
        "## METEOR\n",
        "- like bleu in overall principle, with many other tricks, like consider paraphrases, reordering, and function word/content word diff\n",
        "- pros: generally significantly better than bleu, esp for high-resource languages\n",
        "- cons: requires extra resources for new languages (although these can be made automatically), and more complicated \n",
        "\n",
        "## Embedding-based Metrics (BERTScore, BLEURT)\n",
        "- use either similarity between nerual net embeddings (bertscorer) or a trained nerual network model to predict human scores (BLEURT), or use neural net to predict what a human would say \n",
        "- pros: strong performance (at least on in-domain data) \n",
        "- cons: required trained embeddings, is not interpretable and may not generalize to new domains\n",
        "\n",
        "##perplexity (another method to test our models)\n",
        "- calculat ethe perplexity of the words in theheld-out set without doing generation\n",
        "- Pros: naturally solves multiple-reference problem!\n",
        "- Cons: doesn't consider decoding or actually generating output. so if you had a bug you cannot detect it with perplexity\n",
        "- maybe reasonable for problems with lots of ambiguity like dialogue\n",
        "\n",
        "# Debugging Generation\n",
        "## Debugging Search (1)\n",
        "- your decoding code (search code) should get the same score as loss calculation\n",
        "- test this:\n",
        "  -calculate loss of reference\n",
        "  - perform forced deocoding, where you decode, but tell your model the reference word at each tiem step\n",
        "  - the score of these two should be the same \n",
        "\n",
        "## (2)\n",
        "- as you make search better, the modeol score should get  better  (almost all time)\n",
        "- search with varying beam sizes and make sure you get a better overall model score with larger sizes \n",
        "\n",
        "## Look at your data!\n",
        "- decoding problems can often be detected by looking at outputs and realizing something is weong\n",
        "- ex. the first word of the sentence is droppped every time\n",
        "  - went to he store yesterday\n",
        "  - bought a dog\n",
        "\n",
        "##Quantitative analysis\n",
        "- measure gains quanititaitvely. what is the phenonmenon you chose to focus on? is that phenonmenon getting better? \n",
        "  - you focused on low-ffrequency words: is accuracy on low freqyency words increasing?\n",
        "  - you focused on syntax: is syntax or word ordering getting better, are you doing better on long-distance dependencies? \n",
        "  - you focused on search: are you reducing the number of search errors\n",
        "\n",
        "toolkit: neulab/compare-mt, automatically generates html report, outputs accuracies\n",
        "- read:climbing towards nlu, on meaning, form -- octopus tests\n",
        "# Mismatch b/t optimized function and evaluation metric\n",
        "\n",
        "## loss function, evaluation metric\n",
        "- it is v common to optimize for maximum likelihood in training for that the accuracy can get worse,\n",
        "-A stark example (koehn and knowles 2017) better score (better model score) can result in worse bleu score)\n",
        "- probability is not v correlated with how well the output is\n",
        "\n",
        "## Managing loss functions/eval metric differences\n",
        "- most prrincipled way: use structured prediciotn techniques (in his class)\n",
        "\n",
        "##A simple method: early stopping w/ eval metric\n",
        "- rememberthis graph: difference between number of iterations for best loss vs. best eval \n",
        "\n",
        "# Attention \n",
        "## Sentence representations:\n",
        "- problem: you can't cram the meaning of a whole sentence in a single vector by ray mooney\n",
        "- but what if we could use multiple vectors, based on the length of the sentence\n",
        "- if sentence of length 4, cram it into 4 vectors\n",
        "- more realistic than saying one vector is for a sentence of length 2 and length 40\n",
        "\n",
        "##Basic Idea:\n",
        "- encode each word in the sentence into a vector\n",
        "-when decoding, perfom a linear combination of these vectors, weighted by attention weights - higher attention weight means u focus on this vector more\n",
        "- use this combination in picking thenext word\n",
        "\n",
        "-use 'query' vector (decoder state) and 'key' vectors (all encoder states\n",
        "- for each query-key pair, calculate weight\n",
        "- normalize to add to one using softmax function\n",
        "- results like prob distribution over the words in the input sentence. tells us the first one is v important\n",
        "- then we combine together value vectors (usually encoder states, like key vectors) by taking the weighted sum, and combine them into a single vector\n",
        "- we can use this in any part of the model you like (text classifciation, generation, etc)\n",
        "\n",
        "## Atetention score functions (1) \n",
        "- q is query, k is key\n",
        "- multi-layer perceptron --> feed to neural net with a weight matrix nonlinearlity then another wieight metric\n",
        "  - flexbile, oftern very  good with large data\n",
        "- bilinear \n",
        "  -take the query and multiply by wieght matrix then by key vector\n",
        "- dot product \n",
        "  - no prameters! reuqires sizes to be the same\n",
        "- scaled dot product\n",
        "  -prooblem: scale of dot product increases as dimensions get larger\n",
        "  -fix: scale by size of the vector \n",
        "\n",
        "- let's try it out attention.py\n",
        "\n",
        "# improvements to attention and transformers\n",
        "## intra-attention/ self attention\n",
        "- each element in the setnece attends to other elements -< context senestivie encodings\n",
        "- row and columns are identical, you usually focus on yourself but you can pull in information from other words\n",
        "\n",
        "##Multi-header attention\n",
        "- idea: multiple attention \"heads\" focus on different parts  of the sentence\n",
        "- eg. different heads for \"copy\" vs regular\n",
        "- or multiple independently learned heads (what the transformer paper did)\n",
        "- or one head for every hiden node (too comp. expensive)\n",
        "\n",
        "## Summary of the \"transformer\"\n",
        "-a sequence to sequence model based entirely on attention\n",
        "- strong results on standard WMT datasets\n",
        "- fast: only matrix multiplication\n",
        "\n",
        "## Attention tricks\n",
        "- **self attention**: each layer combines words with others\n",
        "- **multi headed attention** : 8 attention heads learned indepen.\n",
        "- **normalziated dot-product attention**: remove vbias in dot produce when using large entworks\n",
        "- **positional encodings**: makes sure that even if we dont have RNN still can distringus psition allows you to distinguish between \"the\" in the first psotion or in the firfth possiton\n",
        "\n",
        "## Trianing tricks\n",
        "-**layered normaliztion**: help ensure that layers remain in reasonabl range\n",
        "- **specialized training schedule**: adjust default learning rate of the adam optimizer (dec learning late afteR) \n",
        "- **label smoothing**:  insert some uncertainty in training process, by smoothing out prob of all the things you can predict\n",
        "- **masking for efficient training** \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG7jfCZIvo00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}